procgen-apex:
    env: stacked_procgen_env  # Change this at your own risk :D
    run: APEX
    stop:
        timesteps_total: 8000000
    checkpoint_freq: 100
    checkpoint_at_end: True
    keep_checkpoints_num: 5
    # num_samples: 3

    config:
        # ENV
        env_config:
          env_name: coinrun
          num_levels: 0
          start_level: 0
          paint_vel_info: False
          use_generated_assets: False
          center_agent: True
          use_sequential_levels: False
          distribution_mode: easy

        # DQN
        double_q: true   # Use
        dueling: true    # Use
        # Number of atoms for representing the distribution of return. When
        # this is greater than 1, distributional Q-learning is used.
        # the discrete supports are bounded by v_min and v_max
        num_atoms: 99     # 5 PBT: "tune.sample_from(lambda s: random.randint(1,100))"
        v_min: -5.0
        v_max: 20.0
        noisy: false

        # OPTIM
        n_step: 20       # 3 PBT: "tune.sample_from(lambda s: random.randint(1,20))"
        lr: 1.0e-4      # 1.0e-4 PBT: "tune.sample_from(lambda s: random.uniform(5e-5, 1e-3))"
        adam_epsilon: 0.00015
        grad_clip: 40
        learning_starts: 50000

        # NETWORK
        hiddens: [512]

        # PER
        buffer_size: 200000        # Increase on the server
        # compress_observations: True
        prioritized_replay_alpha: 0.5
        final_prioritized_replay_beta: 1.0
        prioritized_replay_beta: 0.4
        prioritized_replay_beta_annealing_timesteps: 2000000

        # EXPLORATION
        # TODO: Try other exploration methods (Soft-Q,...)
        exploration_config:
          final_epsilon: 0.01
          epsilon_timesteps: 20000

        # MODEL
        model:
          # dim: [64,64]
          # conv_filters: [16,32,32]
          custom_model: impala_cnn_tf
          # Extra options to pass to custom class
          custom_options: #custom_options,custom_model_config
              framestack: True
              cnn_layers:
                  - [16, 2]
                  - [32, 2]
                  - [32, 2]

        # APEX
        num_gpus: 0.5
        num_workers: 6
        num_gpus_per_worker: 0.05
        num_envs_per_worker: 4

        # Update the replay buffer with this many samples at once. Note that
        # this setting applies per-worker if num_workers > 1.
        rollout_fragment_length: 16
        # Size of a batch sampled from replay buffer for training. Note that
        # if async_updates is set, then each worker returns gradients for a
        # batch of this size.
        train_batch_size: 128    # 128 PBT: "tune.sample_from(lambda s: random.randint(16,512))"
        target_network_update_freq: 20000
        timesteps_per_iteration: 10000
